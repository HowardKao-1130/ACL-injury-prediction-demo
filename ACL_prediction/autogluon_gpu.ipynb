{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.core.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import shap\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {device_name}\")\n",
    "else: \n",
    "    print('CPU')\n",
    "\n",
    "import json\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "time_limit = 300000\n",
    "\n",
    "metric = 'f1'\n",
    "metric = 'balanced_accuracy'\n",
    "\n",
    "presets = 'best_quality'\n",
    "presets = 'medium_quality'\n",
    "presets = 'extreme'\n",
    "\n",
    "random_seed_base = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_scorer_func(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2, average='binary', zero_division=0)\n",
    "\n",
    "f2_scorer = make_scorer(\n",
    "    score_func=partial(fbeta_score, beta=2),\n",
    "    name='f2',\n",
    "    greater_is_better=True,\n",
    "    needs_proba=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63614f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogluonWrapper:\n",
    "    def __init__(self, predictor, feature_names):\n",
    "        self.ag_model = predictor\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def predict_binary_prob(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.values.reshape(1,-1)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names)\n",
    "        return self.ag_model.predict_proba(X, as_multiclass=False)\n",
    "\n",
    "def plot_shap_beeswarm(train_val_df, test_df, predictor, metric, random_seed):\n",
    "    X_train_val = train_val_df.iloc[:, :-1]\n",
    "    y_train_val = train_val_df.iloc[:, -1]\n",
    "    X_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    # X = df.iloc[:, :-1]\n",
    "\n",
    "    feature_names = X_train_val.columns\n",
    "\n",
    "    print(\"positive class:\", predictor.positive_class)\n",
    "\n",
    "    med = X_train_val.median()\n",
    "    ag_wrapper = AutogluonWrapper(predictor, feature_names)\n",
    "    explainer = shap.KernelExplainer(ag_wrapper.predict_binary_prob, med)\n",
    "    shap_values_arr = explainer.shap_values(X_test.iloc[:,:])\n",
    "\n",
    "    shap.summary_plot(shap_values_arr, X_test.iloc[:,:], show=False)\n",
    "    fig = plt.gcf()\n",
    "    fig.savefig(f'../AutoGluon Figures/beeswarm_{metric}_{random_seed}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return(np.mean(np.abs(shap_values_arr), axis=0))\n",
    "\n",
    "def plot_shap_force(train_val_df, test_df, predictor, metric, random_seed):\n",
    "    X_train_val = train_val_df.iloc[:, :-1]\n",
    "    y_train_val = train_val_df.iloc[:, -1]\n",
    "    X_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "    # X = df.iloc[:, :-1]\n",
    "\n",
    "    feature_names = X_train_val.columns\n",
    "\n",
    "    print(\"positive class:\", predictor.positive_class)\n",
    "\n",
    "    med = X_train_val.median()\n",
    "    ag_wrapper = AutogluonWrapper(predictor, feature_names)\n",
    "    explainer = shap.KernelExplainer(ag_wrapper.predict_binary_prob, med)\n",
    "    shap_values_arr = explainer.shap_values(X_test.iloc[:,:])\n",
    "    shap.force_plot(explainer.expected_value, shap_values_arr, X_test.iloc[:,:])\n",
    "\n",
    "def plot_shap_bar(shap_values, shap_values_default_seed, test_df, metric):\n",
    "    sorted_indices = np.argsort(np.abs(shap_values))[::-1]\n",
    "    sorted_shap_values = shap_values[sorted_indices]\n",
    "    sorted_shap_values_default_seed = shap_values_default_seed[sorted_indices]\n",
    "    feature_names = test_df.columns\n",
    "    sorted_features = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    bar_width = 0.35\n",
    "    y_positions = np.arange(len(sorted_shap_values))\n",
    "\n",
    "    for i, (val, val_default) in enumerate(zip(sorted_shap_values, sorted_shap_values_default_seed)):\n",
    "        abs_val = abs(val)\n",
    "        abs_val_default = abs(val_default)\n",
    "\n",
    "        if abs_val >= abs_val_default:\n",
    "            ax.barh(y_positions[i] - bar_width/2, val, bar_width, color='skyblue', label='SHAP Values (Average across all 10 seeds)' if i == 0 else '')\n",
    "            ax.barh(y_positions[i] + bar_width/2, val_default, bar_width, color='lightcoral', label='SHAP Values (Seed 42)' if i == 0 else '')\n",
    "        else:\n",
    "            ax.barh(y_positions[i] + bar_width/2, val_default, bar_width, color='lightcoral', label='SHAP Values (Seed 42)' if i == 0 else '')\n",
    "            ax.barh(y_positions[i] - bar_width/2, val, bar_width, color='skyblue', label='SHAP Values (Average across all 10 seeds)' if i == 0 else '')\n",
    "\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(sorted_features)\n",
    "    ax.set_xlabel('SHAP Value Average Magnitude')\n",
    "    ax.set_title(f'Feature Importance Based on Average SHAP Value Magnitude ({metric})')\n",
    "    ax.invert_yaxis()\n",
    "    ax.legend()  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../AutoGluon Figures/bar_{metric}.png', dpi=600, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = [random_seed_base + 10 * modifier for modifier in range(10)]\n",
    "metrics = ['balanced_accuracy', 'f1', 'f2']\n",
    "eval_metrics = ['accuracy', 'balanced_accuracy', 'f1', f2_scorer, 'roc_auc', 'precision', 'recall']\n",
    "\n",
    "df = pd.read_csv('../Datasets/tibial_slope_2.csv')\n",
    "df = df.drop(columns=['Subject #'])\n",
    "df['Sex'] = df['Sex'].map({'F': 0, 'M': 1})\n",
    "\n",
    "label = 'Injury'\n",
    "best_model = 'WeightedEnsemble_L2'\n",
    "\n",
    "summary_dir = f'../AutoGluon Summary'\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "figure_dir = f'../AutoGluon Figures'\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "\n",
    "for metric in metrics:\n",
    "    leaderboards_per_metric = []\n",
    "    shap_values_per_metric = []\n",
    "    for random_seed in random_seeds:\n",
    "        print('============================================')\n",
    "        print(f'Metric: {metric}, Random Seed: {random_seed}')\n",
    "        print('============================================')\n",
    "        predictor_dir = f'../AutoGluon Models/AutoGluon_{metric}_{random_seed}'\n",
    "        \n",
    "        train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df[label])\n",
    "        \n",
    "        if os.path.exists(predictor_dir):\n",
    "            predictor = TabularPredictor.load(predictor_dir)\n",
    "        else:\n",
    "            predictor = TabularPredictor(\n",
    "                label=label,\n",
    "                problem_type='binary',\n",
    "                eval_metric=metric if metric != 'f2' else f2_scorer,\n",
    "                path=predictor_dir\n",
    "            )\n",
    "            predictor.fit(\n",
    "                train_data=train_val_df,\n",
    "                time_limit=time_limit,\n",
    "                presets=presets,\n",
    "                num_gpus=1\n",
    "            )\n",
    "\n",
    "        lb_test = predictor.leaderboard(test_df, extra_metrics=eval_metrics, silent=True).iloc[:, :10]\n",
    "        columns = lb_test.columns.tolist()\n",
    "        new_column_order = [columns[0], 'score_val'] + [col for col in columns[1:] if col not in ['score_val', 'score_test']]\n",
    "        lb_test = lb_test[new_column_order]\n",
    "        lb_test = lb_test.rename(columns = {'score_val': f'val {metric}'})\n",
    "        new_columns_names = [col if i < 2 else f'test {col}' for i, col in enumerate(lb_test.columns)]\n",
    "        lb_test.columns = new_columns_names\n",
    "\n",
    "        leaderboards_per_metric.append(lb_test)\n",
    "        shap_value = plot_shap_beeswarm(train_val_df, test_df, predictor, metric, random_seed)\n",
    "        plot_shap_force(train_val_df, test_df, predictor, metric, random_seed)\n",
    "        shap_values_per_metric.append(shap_value)\n",
    "\n",
    "    print('======================================================')\n",
    "    print(f'Summary ({metric})')\n",
    "    print('======================================================')\n",
    "\n",
    "    all_rows = []\n",
    "    for idx, leader_board in enumerate(leaderboards_per_metric):\n",
    "        row = leader_board[leader_board['model'] == best_model]\n",
    "        val_column = [col for col in leader_board.columns if col.startswith('val')][0]\n",
    "        row = row.rename(columns = {'model': f'random seed'})\n",
    "        row['random seed'] = random_seeds[idx]\n",
    "        row = row.drop(columns='test accuracy')\n",
    "        all_rows.append(row)\n",
    "    combined_rows = pd.concat(all_rows, ignore_index=True)\n",
    "    summary_path = os.path.join(summary_dir, f'AutoGluon_{metric}_seeds.csv')\n",
    "    combined_rows.to_csv(summary_path, index=False)\n",
    "\n",
    "    all_lb = pd.concat(leaderboards_per_metric, ignore_index=True)\n",
    "    aggregated_performance = all_lb.groupby('model').agg(\n",
    "        avg_val_score=(f'val {metric}', 'mean'),\n",
    "        std_val_score=(f'val {metric}', 'std'),\n",
    "        avg_testbalanced_accuracy=('test balanced_accuracy', 'mean'),\n",
    "        std_testbalanced_accuracy=('test balanced_accuracy', 'std'),\n",
    "        avg_test_F1=('test f1', 'mean'),\n",
    "        std_test_F1=('test f1', 'std'),\n",
    "        avg_test_F2=('test f2', 'mean'),\n",
    "        std_test_F2=('test f2', 'std'),\n",
    "        avg_test_AUC=('test roc_auc', 'mean'),\n",
    "        std_test_AUC=('test roc_auc', 'std'),\n",
    "        avg_test_precision=('test precision', 'mean'),\n",
    "        std_test_precision=('test precision', 'std'),\n",
    "        avg_test_recall=('test recall', 'mean'),\n",
    "        std_test_recall=('test recall', 'std'),        \n",
    "        count=('model', 'size')\n",
    "    ).sort_values(by='avg_val_score', ascending=False)\n",
    "    aggregated_performance = aggregated_performance.reset_index()\n",
    "    print(f'Summary for {metric}:\\n', aggregated_performance)\n",
    "\n",
    "    summary_path = os.path.join(summary_dir, f'AutoGluon_{metric}.csv')\n",
    "    aggregated_performance.to_csv(summary_path, index=False)\n",
    "\n",
    "    shap_values_per_metric = np.array(shap_values_per_metric)\n",
    "    shap_values_default_seed = shap_values_per_metric[0]\n",
    "    shap_values_per_metric = np.mean(shap_values_per_metric, axis=0)\n",
    "    plot_shap_bar(shap_values_per_metric, shap_values_default_seed, test_df, metric)\n",
    "\n",
    "    predictor_dir = f'../AutoGluon Models/AutoGluon_{metric}_Full'    \n",
    "    if os.path.exists(predictor_dir):\n",
    "        predictor = TabularPredictor.load(predictor_dir)\n",
    "    else:\n",
    "        predictor = TabularPredictor(\n",
    "            label=label,\n",
    "            problem_type='binary',\n",
    "            eval_metric=metric if metric != 'f2' else f2_scorer,\n",
    "            path=predictor_dir\n",
    "        )\n",
    "        predictor.fit(\n",
    "            train_data=df,\n",
    "            time_limit=time_limit,\n",
    "            presets=presets,\n",
    "            num_gpus=1\n",
    "        )\n",
    "    print(predictor.leaderboard())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_gluon_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
